{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Var4BBEd1IXI"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "punctuations = string.punctuation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_lIJLrB2VCm",
        "outputId": "2c7040d7-cac9-413e-e359-f6e57a45fffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "sent = \"The main challenge, is to start!\"\n",
        "stop = stopwords.words('english') + list(punctuations)\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "vectorizer = TfidfVectorizer(stop_words=stop) #stop was defined initially using stopwords from NLTK\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "import re                                  #regular expression\n",
        "from collections import Counter            #creating frequency count dict\n",
        "import heapq                               #for selecting n largest\n",
        "import os\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "WORDS = Counter(words(open('/content/gdrive/MyDrive/t8.shakespeare.txt').read()))\n",
        "def P(word, N=sum(WORDS.values())):\n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "def correction(word):\n",
        "    \"Most probable spelling correction for word.\"\n",
        "    listProb = {word: P(word) for word in candidates(word)}\n",
        "    return listProb, max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words):\n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word):\n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "def get_correct_word(word):\n",
        "    corrected_word = next(iter(correction(word)[0]))\n",
        "    print(\"Word passed: \", word, \" Word corrected To: \", corrected_word)\n",
        "    return corrected_word\n",
        "s=input(\"Enter sentence to be checked \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNXHByEI1PjU",
        "outputId": "c989ff0f-b699-467d-c439-18bc0f967d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter sentence to be checked mighy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=s.split()\n",
        "corrected=[]\n",
        "for i in x:\n",
        "  c=get_correct_word(i)\n",
        "  corrected.append(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEV1Fg4p2NiX",
        "outputId": "edf53e45-95dc-40a9-f787-474c19cad42d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word passed:  mighy  Word corrected To:  mighty\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-EcrlvW2lJb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}